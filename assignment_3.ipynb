{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = '../../Data/Tutorial/notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Solution 1\n",
    "---------\n",
    "\n",
    "The idea is to find all the trainable variables and calculate l2_loss using built-in `tf.nn.l2_loss` function. Then add all of them to the actual loss, so that optimizer finds a way to minimize it too. Source https://stackoverflow.com/a/38466108/5330223"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_layer_size = 250\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default() :\n",
    "    \n",
    "    # Layer 1\n",
    "    weights_layer_1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_layer_size]), name=\"weight_layer_1\")\n",
    "    biases_layer_1 = tf.Variable(tf.zeros(hidden_layer_size), name=\"biases_layer_1\")\n",
    "    \n",
    "    # Layer 2\n",
    "    weights_layer_2 = tf.Variable(tf.truncated_normal([hidden_layer_size, num_labels]), name=\"weight_layer_2\")\n",
    "    biases_layer_2 = tf.Variable(tf.zeros(num_labels), name=\"biases_layer_2\")\n",
    "    \n",
    "    def model(tf_train_dataset) : \n",
    "        out_layer_1 = tf.matmul(tf_train_dataset, weights_layer_1) + biases_layer_1\n",
    "        #dense_layer = tf.layers.dense(inputs=out_layer_1, units=hidden_layer_size, activation=tf.nn.relu)\n",
    "        return tf.matmul(tf.nn.relu(out_layer_1), weights_layer_2) + biases_layer_2\n",
    "    \n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labelset = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    \n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    logits = model(tf_train_dataset)\n",
    "    \n",
    "    #calculate l2 regularization loss\n",
    "    vars   = tf.trainable_variables()\n",
    "    lossL2 = tf.add_n([ tf.nn.l2_loss(v) for v in vars ]) * 0.001\n",
    "    \n",
    "    #add l2 regularization loss to actual loss, so that optimizer finds a way to minimize it too.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labelset)) + lossL2\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(loss)\n",
    "    train_predictions = tf.nn.softmax(logits=logits)\n",
    "    valid_predictions = tf.nn.softmax(logits=model(tf_valid_dataset))\n",
    "    test_predictions = tf.nn.softmax(logits=model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it improves the score by a bit. Same thing without regularization in assignment 2 got about 86 (<90) on test set, here it's greater than 90. See below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized...\n",
      "Loss after mini batch step 0 is : 280.232422\n",
      "Mini batch accuracy : 4.687500\n",
      "Validation accuracy : 21.140000\n",
      "Loss after mini batch step 500 is : 48.212273\n",
      "Mini batch accuracy : 66.406250\n",
      "Validation accuracy : 74.970000\n",
      "Loss after mini batch step 1000 is : 28.050369\n",
      "Mini batch accuracy : 78.125000\n",
      "Validation accuracy : 81.090000\n",
      "Loss after mini batch step 1500 is : 17.036791\n",
      "Mini batch accuracy : 82.031250\n",
      "Validation accuracy : 82.990000\n",
      "Loss after mini batch step 2000 is : 10.930830\n",
      "Mini batch accuracy : 79.687500\n",
      "Validation accuracy : 83.580000\n",
      "Loss after mini batch step 2500 is : 6.759264\n",
      "Mini batch accuracy : 79.687500\n",
      "Validation accuracy : 85.800000\n",
      "Loss after mini batch step 3000 is : 4.049703\n",
      "Mini batch accuracy : 88.281250\n",
      "Validation accuracy : 86.060000\n",
      "Test accuracy : 91.890000\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as sess :\n",
    "    tf.global_variables_initializer().run()\n",
    "    print (\"Initialized...\")\n",
    "    \n",
    "    for step in range(num_steps) :\n",
    "        \n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        batch_data = train_dataset[offset:offset + batch_size, : ]\n",
    "        batch_labels = train_labels[offset:offset + batch_size, : ]\n",
    "        \n",
    "        feed_dict = {\n",
    "            tf_train_dataset : batch_data\n",
    "            , tf_train_labelset : batch_labels\n",
    "        }\n",
    "        \n",
    "        _, l, predictions = sess.run([optimizer, loss, train_predictions], feed_dict)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if (step % 500 == 0) :\n",
    "            print (\"Loss after mini batch step %d is : %f\" % (step, l))\n",
    "            print (\"Mini batch accuracy : %f\" % accuracy(predictions, batch_labels))\n",
    "            print (\"Validation accuracy : %f\" % accuracy(valid_predictions.eval(), valid_labels))\n",
    "    print (\"Test accuracy : %f\" % accuracy(test_predictions.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized...\n",
      "Loss after mini batch step 0 is : 242.390121\n",
      "Mini batch accuracy : 15.625000\n",
      "Validation accuracy : 27.590000\n",
      "Loss after mini batch step 500 is : 46.527313\n",
      "Mini batch accuracy : 100.000000\n",
      "Validation accuracy : 75.050000\n",
      "Loss after mini batch step 1000 is : 28.218557\n",
      "Mini batch accuracy : 100.000000\n",
      "Validation accuracy : 75.110000\n",
      "Loss after mini batch step 1500 is : 17.116915\n",
      "Mini batch accuracy : 100.000000\n",
      "Validation accuracy : 75.490000\n",
      "Loss after mini batch step 2000 is : 10.387437\n",
      "Mini batch accuracy : 100.000000\n",
      "Validation accuracy : 76.110000\n",
      "Loss after mini batch step 2500 is : 6.311393\n",
      "Mini batch accuracy : 100.000000\n",
      "Validation accuracy : 76.920000\n",
      "Loss after mini batch step 3000 is : 3.845309\n",
      "Mini batch accuracy : 100.000000\n",
      "Validation accuracy : 77.990000\n",
      "Test accuracy : 84.630000\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as sess :\n",
    "    tf.global_variables_initializer().run()\n",
    "    print (\"Initialized...\")\n",
    "    \n",
    "    for step in range(num_steps) :\n",
    "        \n",
    "        offset = ((step % 10) * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        batch_data = train_dataset[offset:offset + batch_size, : ]\n",
    "        batch_labels = train_labels[offset:offset + batch_size, : ]\n",
    "        \n",
    "        feed_dict = {\n",
    "            tf_train_dataset : batch_data\n",
    "            , tf_train_labelset : batch_labels\n",
    "        }\n",
    "        \n",
    "        _, l, predictions = sess.run([optimizer, loss, train_predictions], feed_dict)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if (step % 500 == 0) :\n",
    "            print (\"Loss after mini batch step %d is : %f\" % (step, l))\n",
    "            print (\"Mini batch accuracy : %f\" % accuracy(predictions, batch_labels))\n",
    "            print (\"Validation accuracy : %f\" % accuracy(valid_predictions.eval(), valid_labels))\n",
    "    print (\"Test accuracy : %f\" % accuracy(test_predictions.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected reducing training set to 1st 10 batches overfitting arises, meaning train set gets 100% accuracy but validation and train set shows poor result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Designing network for dropout only on training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_layer_size = 250\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default() :\n",
    "    \n",
    "    # Layer 1\n",
    "    weights_layer_1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_layer_size]), name=\"weight_layer_1\")\n",
    "    biases_layer_1 = tf.Variable(tf.zeros(hidden_layer_size), name=\"biases_layer_1\")\n",
    "    \n",
    "    # Layer 2\n",
    "    weights_layer_2 = tf.Variable(tf.truncated_normal([hidden_layer_size, num_labels]), name=\"weight_layer_2\")\n",
    "    biases_layer_2 = tf.Variable(tf.zeros(num_labels), name=\"biases_layer_2\")\n",
    "    \n",
    "    def model(tf_train_dataset, training = True) : \n",
    "        out_layer_1 = tf.matmul(tf_train_dataset, weights_layer_1) + biases_layer_1\n",
    "        relu_layer = tf.nn.relu(out_layer_1)\n",
    "        if (training == True) :\n",
    "            relu_layer = tf.nn.dropout(relu_layer, keep_prob=0.5)\n",
    "        return tf.matmul(relu_layer, weights_layer_2) + biases_layer_2\n",
    "    \n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labelset = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    \n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    logits = model(tf_train_dataset)\n",
    "    \n",
    "    #calculate l2 regularization loss\n",
    "    vars   = tf.trainable_variables()\n",
    "    lossL2 = tf.add_n([ tf.nn.l2_loss(v) for v in vars ]) * 0.001\n",
    "    \n",
    "    #add l2 regularization loss to actual loss, so that optimizer finds a way to minimize it too.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labelset)) + lossL2\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(loss)\n",
    "    train_predictions = tf.nn.softmax(logits=logits)\n",
    "    valid_predictions = tf.nn.softmax(logits=model(tf_valid_dataset, False))\n",
    "    test_predictions = tf.nn.softmax(logits=model(tf_test_dataset, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now start training normally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized...\n",
      "Loss after mini batch step 0 is : 368.047760\n",
      "Mini batch accuracy : 5.468750\n",
      "Validation accuracy : 36.120000\n",
      "Loss after mini batch step 500 is : 46.534801\n",
      "Mini batch accuracy : 96.093750\n",
      "Validation accuracy : 78.720000\n",
      "Loss after mini batch step 1000 is : 28.358191\n",
      "Mini batch accuracy : 97.656250\n",
      "Validation accuracy : 79.190000\n",
      "Loss after mini batch step 1500 is : 17.160471\n",
      "Mini batch accuracy : 98.437500\n",
      "Validation accuracy : 79.810000\n",
      "Loss after mini batch step 2000 is : 10.407832\n",
      "Mini batch accuracy : 99.218750\n",
      "Validation accuracy : 80.110000\n",
      "Loss after mini batch step 2500 is : 6.344682\n",
      "Mini batch accuracy : 99.218750\n",
      "Validation accuracy : 80.350000\n",
      "Loss after mini batch step 3000 is : 3.876447\n",
      "Mini batch accuracy : 98.437500\n",
      "Validation accuracy : 80.630000\n",
      "Test accuracy : 86.790000\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as sess :\n",
    "    tf.global_variables_initializer().run()\n",
    "    print (\"Initialized...\")\n",
    "    \n",
    "    for step in range(num_steps) :\n",
    "        \n",
    "        offset = ((step % 10) * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        batch_data = train_dataset[offset:offset + batch_size, : ]\n",
    "        batch_labels = train_labels[offset:offset + batch_size, : ]\n",
    "        \n",
    "        feed_dict = {\n",
    "            tf_train_dataset : batch_data\n",
    "            , tf_train_labelset : batch_labels\n",
    "        }\n",
    "        \n",
    "        _, l, predictions = sess.run([optimizer, loss, train_predictions], feed_dict)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if (step % 500 == 0) :\n",
    "            print (\"Loss after mini batch step %d is : %f\" % (step, l))\n",
    "            print (\"Mini batch accuracy : %f\" % accuracy(predictions, batch_labels))\n",
    "            print (\"Validation accuracy : %f\" % accuracy(valid_predictions.eval(), valid_labels))\n",
    "    print (\"Test accuracy : %f\" % accuracy(test_predictions.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Little imporvement while using dropout than the previous overfit case. As we are still training on only 1st 10 batch of the data which had 77 and 84% accuracy on validation and test sets correspondingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using dropout and regularization as the previous problem, increasing batch size and hidden layer size and making it deep. Adding learning rate decay too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_layer_size_1 = 256\n",
    "hidden_layer_size_2 = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default() :\n",
    "    \n",
    "    # Layer 1\n",
    "    weights_layer_1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_layer_size_1]), name=\"weight_layer_1\")\n",
    "    biases_layer_1 = tf.Variable(tf.zeros(hidden_layer_size_1), name=\"biases_layer_1\")\n",
    "    \n",
    "    # Layer 2\n",
    "    weights_layer_2 = tf.Variable(tf.truncated_normal([hidden_layer_size_1, hidden_layer_size_2]), name=\"weight_layer_2\")\n",
    "    biases_layer_2 = tf.Variable(tf.zeros(hidden_layer_size_2), name=\"biases_layer_2\")\n",
    "    \n",
    "    # Layer 3\n",
    "    weights_layer_3 = tf.Variable(tf.truncated_normal([hidden_layer_size_2, num_labels]), name=\"weight_layer_3\")\n",
    "    biases_layer_3 = tf.Variable(tf.zeros(num_labels), name=\"biases_layer_3\")\n",
    "    \n",
    "    def model(tf_train_dataset, training = True) : \n",
    "        out_layer_1 = tf.matmul(tf_train_dataset, weights_layer_1) + biases_layer_1\n",
    "        relu_layer1 = tf.nn.relu(out_layer_1)\n",
    "        if (training == True) :\n",
    "            relu_layer1 = tf.nn.dropout(relu_layer1, keep_prob=0.8)\n",
    "            \n",
    "        out_layer_2 = tf.matmul(relu_layer1, weights_layer_2) + biases_layer_2\n",
    "        relu_layer2 = tf.nn.relu(out_layer_2)\n",
    "        if (training == True) :\n",
    "            relu_layer2 = tf.nn.dropout(relu_layer2, keep_prob=0.6)\n",
    "        return tf.matmul(relu_layer2, weights_layer_3) + biases_layer_3\n",
    "    \n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labelset = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    \n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    logits = model(tf_train_dataset)\n",
    "    \n",
    "    #calculate l2 regularization loss\n",
    "    vars   = tf.trainable_variables()\n",
    "    lossL2 = tf.add_n([ tf.nn.l2_loss(v) for v in vars ]) * 0.001\n",
    "    \n",
    "    #add l2 regularization loss to actual loss, so that optimizer finds a way to minimize it too.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labelset)) + lossL2\n",
    "    \n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    starter_learning_rate = 0.1\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                               100000, 0.90, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    train_predictions = tf.nn.softmax(logits=logits)\n",
    "    valid_predictions = tf.nn.softmax(logits=model(tf_valid_dataset, False))\n",
    "    test_predictions = tf.nn.softmax(logits=model(tf_test_dataset, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train it longer time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized...\n",
      "Loss after mini batch step 0 is : 1846.132568 learning rate is : 0.100000\n",
      "Mini batch accuracy : 10.156250\n",
      "Validation accuracy : 12.770000\n",
      "Loss after mini batch step 1000 is : 79.591560 learning rate is : 0.100000\n",
      "Mini batch accuracy : 12.500000\n",
      "Validation accuracy : 12.720000\n",
      "Loss after mini batch step 2000 is : 65.594231 learning rate is : 0.100000\n",
      "Mini batch accuracy : 13.281250\n",
      "Validation accuracy : 15.480000\n",
      "Loss after mini batch step 3000 is : 53.917740 learning rate is : 0.100000\n",
      "Mini batch accuracy : 19.531250\n",
      "Validation accuracy : 18.160000\n",
      "Loss after mini batch step 4000 is : 44.463432 learning rate is : 0.100000\n",
      "Mini batch accuracy : 17.968750\n",
      "Validation accuracy : 19.990000\n",
      "Loss after mini batch step 5000 is : 37.034874 learning rate is : 0.100000\n",
      "Mini batch accuracy : 25.000000\n",
      "Validation accuracy : 34.740000\n",
      "Loss after mini batch step 6000 is : 30.103436 learning rate is : 0.100000\n",
      "Mini batch accuracy : 33.593750\n",
      "Validation accuracy : 36.380000\n",
      "Loss after mini batch step 7000 is : 24.867954 learning rate is : 0.100000\n",
      "Mini batch accuracy : 42.968750\n",
      "Validation accuracy : 48.780000\n",
      "Loss after mini batch step 8000 is : 20.778389 learning rate is : 0.100000\n",
      "Mini batch accuracy : 47.656250\n",
      "Validation accuracy : 57.060000\n",
      "Loss after mini batch step 9000 is : 16.890476 learning rate is : 0.100000\n",
      "Mini batch accuracy : 59.375000\n",
      "Validation accuracy : 67.880000\n",
      "Loss after mini batch step 10000 is : 13.973796 learning rate is : 0.100000\n",
      "Mini batch accuracy : 57.031250\n",
      "Validation accuracy : 72.510000\n",
      "Loss after mini batch step 11000 is : 11.707930 learning rate is : 0.100000\n",
      "Mini batch accuracy : 57.812500\n",
      "Validation accuracy : 79.810000\n",
      "Loss after mini batch step 12000 is : 9.478515 learning rate is : 0.100000\n",
      "Mini batch accuracy : 67.187500\n",
      "Validation accuracy : 80.780000\n",
      "Loss after mini batch step 13000 is : 7.831523 learning rate is : 0.100000\n",
      "Mini batch accuracy : 72.656250\n",
      "Validation accuracy : 81.990000\n",
      "Loss after mini batch step 14000 is : 6.345570 learning rate is : 0.100000\n",
      "Mini batch accuracy : 75.781250\n",
      "Validation accuracy : 83.130000\n",
      "Loss after mini batch step 15000 is : 5.278355 learning rate is : 0.100000\n",
      "Mini batch accuracy : 81.250000\n",
      "Validation accuracy : 83.490000\n",
      "Loss after mini batch step 16000 is : 4.565092 learning rate is : 0.100000\n",
      "Mini batch accuracy : 78.125000\n",
      "Validation accuracy : 84.350000\n",
      "Loss after mini batch step 17000 is : 3.916151 learning rate is : 0.100000\n",
      "Mini batch accuracy : 75.000000\n",
      "Validation accuracy : 84.780000\n",
      "Loss after mini batch step 18000 is : 3.385105 learning rate is : 0.100000\n",
      "Mini batch accuracy : 76.562500\n",
      "Validation accuracy : 85.230000\n",
      "Loss after mini batch step 19000 is : 2.872240 learning rate is : 0.100000\n",
      "Mini batch accuracy : 75.000000\n",
      "Validation accuracy : 85.660000\n",
      "Loss after mini batch step 20000 is : 2.327564 learning rate is : 0.100000\n",
      "Mini batch accuracy : 78.125000\n",
      "Validation accuracy : 85.910000\n",
      "Loss after mini batch step 21000 is : 2.028565 learning rate is : 0.100000\n",
      "Mini batch accuracy : 80.468750\n",
      "Validation accuracy : 86.440000\n",
      "Loss after mini batch step 22000 is : 1.657819 learning rate is : 0.100000\n",
      "Mini batch accuracy : 85.156250\n",
      "Validation accuracy : 86.750000\n",
      "Loss after mini batch step 23000 is : 1.413411 learning rate is : 0.100000\n",
      "Mini batch accuracy : 85.937500\n",
      "Validation accuracy : 86.890000\n",
      "Loss after mini batch step 24000 is : 1.216663 learning rate is : 0.100000\n",
      "Mini batch accuracy : 89.843750\n",
      "Validation accuracy : 87.220000\n",
      "Loss after mini batch step 25000 is : 1.219615 learning rate is : 0.100000\n",
      "Mini batch accuracy : 81.250000\n",
      "Validation accuracy : 87.240000\n",
      "Loss after mini batch step 26000 is : 0.968857 learning rate is : 0.100000\n",
      "Mini batch accuracy : 88.281250\n",
      "Validation accuracy : 87.450000\n",
      "Loss after mini batch step 27000 is : 0.953397 learning rate is : 0.100000\n",
      "Mini batch accuracy : 87.500000\n",
      "Validation accuracy : 87.680000\n",
      "Loss after mini batch step 28000 is : 0.873759 learning rate is : 0.100000\n",
      "Mini batch accuracy : 84.375000\n",
      "Validation accuracy : 88.200000\n",
      "Loss after mini batch step 29000 is : 0.733924 learning rate is : 0.100000\n",
      "Mini batch accuracy : 89.843750\n",
      "Validation accuracy : 88.440000\n",
      "Loss after mini batch step 30000 is : 0.603180 learning rate is : 0.100000\n",
      "Mini batch accuracy : 90.625000\n",
      "Validation accuracy : 88.090000\n",
      "Loss after mini batch step 31000 is : 0.699296 learning rate is : 0.100000\n",
      "Mini batch accuracy : 87.500000\n",
      "Validation accuracy : 88.620000\n",
      "Loss after mini batch step 32000 is : 0.722780 learning rate is : 0.100000\n",
      "Mini batch accuracy : 85.156250\n",
      "Validation accuracy : 88.700000\n",
      "Loss after mini batch step 33000 is : 0.884894 learning rate is : 0.100000\n",
      "Mini batch accuracy : 76.562500\n",
      "Validation accuracy : 88.770000\n",
      "Loss after mini batch step 34000 is : 0.641178 learning rate is : 0.100000\n",
      "Mini batch accuracy : 86.718750\n",
      "Validation accuracy : 89.150000\n",
      "Loss after mini batch step 35000 is : 0.534557 learning rate is : 0.100000\n",
      "Mini batch accuracy : 90.625000\n",
      "Validation accuracy : 88.970000\n",
      "Loss after mini batch step 36000 is : 0.587985 learning rate is : 0.100000\n",
      "Mini batch accuracy : 86.718750\n",
      "Validation accuracy : 89.230000\n",
      "Loss after mini batch step 37000 is : 0.554428 learning rate is : 0.100000\n",
      "Mini batch accuracy : 85.156250\n",
      "Validation accuracy : 89.230000\n",
      "Loss after mini batch step 38000 is : 0.538315 learning rate is : 0.100000\n",
      "Mini batch accuracy : 87.500000\n",
      "Validation accuracy : 89.400000\n",
      "Loss after mini batch step 39000 is : 0.407075 learning rate is : 0.100000\n",
      "Mini batch accuracy : 91.406250\n",
      "Validation accuracy : 89.450000\n",
      "Loss after mini batch step 40000 is : 0.355375 learning rate is : 0.100000\n",
      "Mini batch accuracy : 92.187500\n",
      "Validation accuracy : 89.520000\n",
      "Loss after mini batch step 41000 is : 0.467618 learning rate is : 0.100000\n",
      "Mini batch accuracy : 91.406250\n",
      "Validation accuracy : 89.470000\n",
      "Loss after mini batch step 42000 is : 0.493575 learning rate is : 0.100000\n",
      "Mini batch accuracy : 89.843750\n",
      "Validation accuracy : 89.680000\n",
      "Loss after mini batch step 43000 is : 0.479908 learning rate is : 0.100000\n",
      "Mini batch accuracy : 89.843750\n",
      "Validation accuracy : 89.280000\n",
      "Loss after mini batch step 44000 is : 0.476930 learning rate is : 0.100000\n",
      "Mini batch accuracy : 88.281250\n",
      "Validation accuracy : 89.630000\n",
      "Loss after mini batch step 45000 is : 0.343320 learning rate is : 0.100000\n",
      "Mini batch accuracy : 92.968750\n",
      "Validation accuracy : 89.580000\n",
      "Loss after mini batch step 46000 is : 0.637393 learning rate is : 0.100000\n",
      "Mini batch accuracy : 82.812500\n",
      "Validation accuracy : 89.640000\n",
      "Loss after mini batch step 47000 is : 0.446709 learning rate is : 0.100000\n",
      "Mini batch accuracy : 89.062500\n",
      "Validation accuracy : 89.680000\n",
      "Loss after mini batch step 48000 is : 0.343927 learning rate is : 0.100000\n",
      "Mini batch accuracy : 93.750000\n",
      "Validation accuracy : 89.780000\n",
      "Loss after mini batch step 49000 is : 0.389208 learning rate is : 0.100000\n",
      "Mini batch accuracy : 91.406250\n",
      "Validation accuracy : 89.690000\n",
      "Loss after mini batch step 50000 is : 0.518807 learning rate is : 0.100000\n",
      "Mini batch accuracy : 89.062500\n",
      "Validation accuracy : 89.850000\n",
      "Loss after mini batch step 51000 is : 0.553740 learning rate is : 0.100000\n",
      "Mini batch accuracy : 84.375000\n",
      "Validation accuracy : 89.690000\n",
      "Loss after mini batch step 52000 is : 0.370330 learning rate is : 0.100000\n",
      "Mini batch accuracy : 91.406250\n",
      "Validation accuracy : 90.050000\n",
      "Loss after mini batch step 53000 is : 0.481278 learning rate is : 0.100000\n",
      "Mini batch accuracy : 92.187500\n",
      "Validation accuracy : 89.700000\n",
      "Loss after mini batch step 54000 is : 0.327241 learning rate is : 0.100000\n",
      "Mini batch accuracy : 95.312500\n",
      "Validation accuracy : 89.880000\n",
      "Loss after mini batch step 55000 is : 0.412472 learning rate is : 0.100000\n",
      "Mini batch accuracy : 91.406250\n",
      "Validation accuracy : 89.850000\n",
      "Loss after mini batch step 56000 is : 0.527908 learning rate is : 0.100000\n",
      "Mini batch accuracy : 87.500000\n",
      "Validation accuracy : 89.810000\n",
      "Loss after mini batch step 57000 is : 0.371675 learning rate is : 0.100000\n",
      "Mini batch accuracy : 92.187500\n",
      "Validation accuracy : 89.790000\n",
      "Loss after mini batch step 58000 is : 0.500103 learning rate is : 0.100000\n",
      "Mini batch accuracy : 88.281250\n",
      "Validation accuracy : 89.700000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini batch step 59000 is : 0.519087 learning rate is : 0.100000\n",
      "Mini batch accuracy : 86.718750\n",
      "Validation accuracy : 90.090000\n",
      "Loss after mini batch step 60000 is : 0.425134 learning rate is : 0.100000\n",
      "Mini batch accuracy : 91.406250\n",
      "Validation accuracy : 89.750000\n",
      "Loss after mini batch step 61000 is : 0.490805 learning rate is : 0.100000\n",
      "Mini batch accuracy : 87.500000\n",
      "Validation accuracy : 89.940000\n",
      "Loss after mini batch step 62000 is : 0.513245 learning rate is : 0.100000\n",
      "Mini batch accuracy : 88.281250\n",
      "Validation accuracy : 90.030000\n",
      "Loss after mini batch step 63000 is : 0.399332 learning rate is : 0.100000\n",
      "Mini batch accuracy : 91.406250\n",
      "Validation accuracy : 89.930000\n",
      "Loss after mini batch step 64000 is : 0.476382 learning rate is : 0.100000\n",
      "Mini batch accuracy : 89.062500\n",
      "Validation accuracy : 89.920000\n",
      "Loss after mini batch step 65000 is : 0.382084 learning rate is : 0.100000\n",
      "Mini batch accuracy : 92.187500\n",
      "Validation accuracy : 90.190000\n",
      "Loss after mini batch step 66000 is : 0.427830 learning rate is : 0.100000\n",
      "Mini batch accuracy : 90.625000\n",
      "Validation accuracy : 89.900000\n",
      "Loss after mini batch step 67000 is : 0.308472 learning rate is : 0.100000\n",
      "Mini batch accuracy : 93.750000\n",
      "Validation accuracy : 90.050000\n",
      "Loss after mini batch step 68000 is : 0.439026 learning rate is : 0.100000\n",
      "Mini batch accuracy : 89.062500\n",
      "Validation accuracy : 89.910000\n",
      "Loss after mini batch step 69000 is : 0.548820 learning rate is : 0.100000\n",
      "Mini batch accuracy : 86.718750\n",
      "Validation accuracy : 89.750000\n",
      "Loss after mini batch step 70000 is : 0.475422 learning rate is : 0.100000\n",
      "Mini batch accuracy : 85.156250\n",
      "Validation accuracy : 90.250000\n",
      "Loss after mini batch step 71000 is : 0.547444 learning rate is : 0.100000\n",
      "Mini batch accuracy : 87.500000\n",
      "Validation accuracy : 89.700000\n",
      "Loss after mini batch step 72000 is : 0.368596 learning rate is : 0.100000\n",
      "Mini batch accuracy : 95.312500\n",
      "Validation accuracy : 90.110000\n",
      "Loss after mini batch step 73000 is : 0.345374 learning rate is : 0.100000\n",
      "Mini batch accuracy : 93.750000\n",
      "Validation accuracy : 89.990000\n",
      "Loss after mini batch step 74000 is : 0.401292 learning rate is : 0.100000\n",
      "Mini batch accuracy : 91.406250\n",
      "Validation accuracy : 90.030000\n",
      "Loss after mini batch step 75000 is : 0.583290 learning rate is : 0.100000\n",
      "Mini batch accuracy : 85.156250\n",
      "Validation accuracy : 89.700000\n",
      "Loss after mini batch step 76000 is : 0.484102 learning rate is : 0.100000\n",
      "Mini batch accuracy : 87.500000\n",
      "Validation accuracy : 90.260000\n",
      "Loss after mini batch step 77000 is : 0.458713 learning rate is : 0.100000\n",
      "Mini batch accuracy : 85.937500\n",
      "Validation accuracy : 90.180000\n",
      "Loss after mini batch step 78000 is : 0.560720 learning rate is : 0.100000\n",
      "Mini batch accuracy : 85.937500\n",
      "Validation accuracy : 90.020000\n",
      "Loss after mini batch step 79000 is : 0.488285 learning rate is : 0.100000\n",
      "Mini batch accuracy : 86.718750\n",
      "Validation accuracy : 90.440000\n",
      "Loss after mini batch step 80000 is : 0.451137 learning rate is : 0.100000\n",
      "Mini batch accuracy : 89.062500\n",
      "Validation accuracy : 89.990000\n",
      "Loss after mini batch step 81000 is : 0.478658 learning rate is : 0.100000\n",
      "Mini batch accuracy : 92.187500\n",
      "Validation accuracy : 90.020000\n",
      "Loss after mini batch step 82000 is : 0.432370 learning rate is : 0.100000\n",
      "Mini batch accuracy : 92.187500\n",
      "Validation accuracy : 90.130000\n",
      "Loss after mini batch step 83000 is : 0.443368 learning rate is : 0.100000\n",
      "Mini batch accuracy : 88.281250\n",
      "Validation accuracy : 90.170000\n",
      "Loss after mini batch step 84000 is : 0.398525 learning rate is : 0.100000\n",
      "Mini batch accuracy : 90.625000\n",
      "Validation accuracy : 90.330000\n",
      "Loss after mini batch step 85000 is : 0.471233 learning rate is : 0.100000\n",
      "Mini batch accuracy : 88.281250\n",
      "Validation accuracy : 90.070000\n",
      "Loss after mini batch step 86000 is : 0.400583 learning rate is : 0.100000\n",
      "Mini batch accuracy : 90.625000\n",
      "Validation accuracy : 90.080000\n",
      "Loss after mini batch step 87000 is : 0.454791 learning rate is : 0.100000\n",
      "Mini batch accuracy : 87.500000\n",
      "Validation accuracy : 90.340000\n",
      "Loss after mini batch step 88000 is : 0.434418 learning rate is : 0.100000\n",
      "Mini batch accuracy : 89.843750\n",
      "Validation accuracy : 90.010000\n",
      "Loss after mini batch step 89000 is : 0.506068 learning rate is : 0.100000\n",
      "Mini batch accuracy : 87.500000\n",
      "Validation accuracy : 90.020000\n",
      "Loss after mini batch step 90000 is : 0.456379 learning rate is : 0.100000\n",
      "Mini batch accuracy : 91.406250\n",
      "Validation accuracy : 90.220000\n",
      "Loss after mini batch step 91000 is : 0.358102 learning rate is : 0.100000\n",
      "Mini batch accuracy : 92.968750\n",
      "Validation accuracy : 89.940000\n",
      "Loss after mini batch step 92000 is : 0.502070 learning rate is : 0.100000\n",
      "Mini batch accuracy : 88.281250\n",
      "Validation accuracy : 90.110000\n",
      "Loss after mini batch step 93000 is : 0.471259 learning rate is : 0.100000\n",
      "Mini batch accuracy : 92.968750\n",
      "Validation accuracy : 90.220000\n",
      "Loss after mini batch step 94000 is : 0.551709 learning rate is : 0.100000\n",
      "Mini batch accuracy : 85.937500\n",
      "Validation accuracy : 90.180000\n",
      "Loss after mini batch step 95000 is : 0.445432 learning rate is : 0.100000\n",
      "Mini batch accuracy : 91.406250\n",
      "Validation accuracy : 90.010000\n",
      "Loss after mini batch step 96000 is : 0.552848 learning rate is : 0.100000\n",
      "Mini batch accuracy : 87.500000\n",
      "Validation accuracy : 90.010000\n",
      "Loss after mini batch step 97000 is : 0.285037 learning rate is : 0.100000\n",
      "Mini batch accuracy : 92.968750\n",
      "Validation accuracy : 90.190000\n",
      "Loss after mini batch step 98000 is : 0.403312 learning rate is : 0.100000\n",
      "Mini batch accuracy : 90.625000\n",
      "Validation accuracy : 90.250000\n",
      "Loss after mini batch step 99000 is : 0.431254 learning rate is : 0.100000\n",
      "Mini batch accuracy : 90.625000\n",
      "Validation accuracy : 90.310000\n",
      "Loss after mini batch step 100000 is : 0.501486 learning rate is : 0.090000\n",
      "Mini batch accuracy : 86.718750\n",
      "Validation accuracy : 90.360000\n",
      "Loss after mini batch step 101000 is : 0.278339 learning rate is : 0.090000\n",
      "Mini batch accuracy : 95.312500\n",
      "Validation accuracy : 90.350000\n",
      "Loss after mini batch step 102000 is : 0.509604 learning rate is : 0.090000\n",
      "Mini batch accuracy : 85.937500\n",
      "Validation accuracy : 90.290000\n",
      "Loss after mini batch step 103000 is : 0.480157 learning rate is : 0.090000\n",
      "Mini batch accuracy : 89.843750\n",
      "Validation accuracy : 90.190000\n",
      "Loss after mini batch step 104000 is : 0.501031 learning rate is : 0.090000\n",
      "Mini batch accuracy : 85.937500\n",
      "Validation accuracy : 90.040000\n",
      "Loss after mini batch step 105000 is : 0.496431 learning rate is : 0.090000\n",
      "Mini batch accuracy : 88.281250\n",
      "Validation accuracy : 90.260000\n",
      "Loss after mini batch step 106000 is : 0.381922 learning rate is : 0.090000\n",
      "Mini batch accuracy : 92.968750\n",
      "Validation accuracy : 90.330000\n",
      "Loss after mini batch step 107000 is : 0.351481 learning rate is : 0.090000\n",
      "Mini batch accuracy : 93.750000\n",
      "Validation accuracy : 90.300000\n",
      "Loss after mini batch step 108000 is : 0.528186 learning rate is : 0.090000\n",
      "Mini batch accuracy : 89.062500\n",
      "Validation accuracy : 90.470000\n",
      "Loss after mini batch step 109000 is : 0.452121 learning rate is : 0.090000\n",
      "Mini batch accuracy : 89.843750\n",
      "Validation accuracy : 90.270000\n",
      "Loss after mini batch step 110000 is : 0.424822 learning rate is : 0.090000\n",
      "Mini batch accuracy : 89.843750\n",
      "Validation accuracy : 90.040000\n",
      "Loss after mini batch step 111000 is : 0.374759 learning rate is : 0.090000\n",
      "Mini batch accuracy : 92.968750\n",
      "Validation accuracy : 90.250000\n",
      "Loss after mini batch step 112000 is : 0.497795 learning rate is : 0.090000\n",
      "Mini batch accuracy : 90.625000\n",
      "Validation accuracy : 90.210000\n",
      "Loss after mini batch step 113000 is : 0.451812 learning rate is : 0.090000\n",
      "Mini batch accuracy : 90.625000\n",
      "Validation accuracy : 90.250000\n",
      "Loss after mini batch step 114000 is : 0.533110 learning rate is : 0.090000\n",
      "Mini batch accuracy : 86.718750\n",
      "Validation accuracy : 90.110000\n",
      "Loss after mini batch step 115000 is : 0.458909 learning rate is : 0.090000\n",
      "Mini batch accuracy : 89.843750\n",
      "Validation accuracy : 89.830000\n",
      "Loss after mini batch step 116000 is : 0.429179 learning rate is : 0.090000\n",
      "Mini batch accuracy : 89.843750\n",
      "Validation accuracy : 90.060000\n",
      "Loss after mini batch step 117000 is : 0.459625 learning rate is : 0.090000\n",
      "Mini batch accuracy : 89.062500\n",
      "Validation accuracy : 90.060000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini batch step 118000 is : 0.480749 learning rate is : 0.090000\n",
      "Mini batch accuracy : 91.406250\n",
      "Validation accuracy : 90.120000\n",
      "Loss after mini batch step 119000 is : 0.473226 learning rate is : 0.090000\n",
      "Mini batch accuracy : 89.843750\n",
      "Validation accuracy : 90.350000\n",
      "Loss after mini batch step 120000 is : 0.408854 learning rate is : 0.090000\n",
      "Mini batch accuracy : 91.406250\n",
      "Validation accuracy : 90.230000\n",
      "Loss after mini batch step 121000 is : 0.459798 learning rate is : 0.090000\n",
      "Mini batch accuracy : 92.187500\n",
      "Validation accuracy : 90.330000\n",
      "Loss after mini batch step 122000 is : 0.502025 learning rate is : 0.090000\n",
      "Mini batch accuracy : 89.843750\n",
      "Validation accuracy : 90.280000\n",
      "Loss after mini batch step 123000 is : 0.470537 learning rate is : 0.090000\n",
      "Mini batch accuracy : 91.406250\n",
      "Validation accuracy : 90.360000\n",
      "Loss after mini batch step 124000 is : 0.477596 learning rate is : 0.090000\n",
      "Mini batch accuracy : 89.062500\n",
      "Validation accuracy : 90.310000\n",
      "Loss after mini batch step 125000 is : 0.414897 learning rate is : 0.090000\n",
      "Mini batch accuracy : 90.625000\n",
      "Validation accuracy : 90.120000\n",
      "Loss after mini batch step 126000 is : 0.473691 learning rate is : 0.090000\n",
      "Mini batch accuracy : 88.281250\n",
      "Validation accuracy : 90.080000\n",
      "Loss after mini batch step 127000 is : 0.398916 learning rate is : 0.090000\n",
      "Mini batch accuracy : 90.625000\n",
      "Validation accuracy : 90.140000\n",
      "Loss after mini batch step 128000 is : 0.477603 learning rate is : 0.090000\n",
      "Mini batch accuracy : 89.843750\n",
      "Validation accuracy : 90.070000\n",
      "Loss after mini batch step 129000 is : 0.545570 learning rate is : 0.090000\n",
      "Mini batch accuracy : 87.500000\n",
      "Validation accuracy : 90.430000\n",
      "Loss after mini batch step 130000 is : 0.479074 learning rate is : 0.090000\n",
      "Mini batch accuracy : 89.062500\n",
      "Validation accuracy : 90.250000\n",
      "Loss after mini batch step 131000 is : 0.419762 learning rate is : 0.090000\n",
      "Mini batch accuracy : 89.843750\n",
      "Validation accuracy : 90.410000\n",
      "Loss after mini batch step 132000 is : 0.523364 learning rate is : 0.090000\n",
      "Mini batch accuracy : 88.281250\n",
      "Validation accuracy : 90.120000\n",
      "Loss after mini batch step 133000 is : 0.456375 learning rate is : 0.090000\n",
      "Mini batch accuracy : 89.062500\n",
      "Validation accuracy : 90.200000\n",
      "Loss after mini batch step 134000 is : 0.435518 learning rate is : 0.090000\n",
      "Mini batch accuracy : 89.062500\n",
      "Validation accuracy : 90.580000\n",
      "Loss after mini batch step 135000 is : 0.425511 learning rate is : 0.090000\n",
      "Mini batch accuracy : 89.843750\n",
      "Validation accuracy : 90.200000\n",
      "Loss after mini batch step 136000 is : 0.372185 learning rate is : 0.090000\n",
      "Mini batch accuracy : 91.406250\n",
      "Validation accuracy : 90.180000\n",
      "Loss after mini batch step 137000 is : 0.558056 learning rate is : 0.090000\n",
      "Mini batch accuracy : 87.500000\n",
      "Validation accuracy : 90.500000\n",
      "Loss after mini batch step 138000 is : 0.415996 learning rate is : 0.090000\n",
      "Mini batch accuracy : 91.406250\n",
      "Validation accuracy : 90.280000\n",
      "Loss after mini batch step 139000 is : 0.388003 learning rate is : 0.090000\n",
      "Mini batch accuracy : 89.062500\n",
      "Validation accuracy : 90.370000\n",
      "Loss after mini batch step 140000 is : 0.567064 learning rate is : 0.090000\n",
      "Mini batch accuracy : 89.062500\n",
      "Validation accuracy : 90.480000\n",
      "Loss after mini batch step 141000 is : 0.478443 learning rate is : 0.090000\n",
      "Mini batch accuracy : 88.281250\n",
      "Validation accuracy : 90.340000\n",
      "Loss after mini batch step 142000 is : 0.359465 learning rate is : 0.090000\n",
      "Mini batch accuracy : 92.187500\n",
      "Validation accuracy : 90.290000\n",
      "Loss after mini batch step 143000 is : 0.369901 learning rate is : 0.090000\n",
      "Mini batch accuracy : 94.531250\n",
      "Validation accuracy : 90.310000\n",
      "Loss after mini batch step 144000 is : 0.466626 learning rate is : 0.090000\n",
      "Mini batch accuracy : 90.625000\n",
      "Validation accuracy : 90.380000\n",
      "Loss after mini batch step 145000 is : 0.472172 learning rate is : 0.090000\n",
      "Mini batch accuracy : 88.281250\n",
      "Validation accuracy : 90.340000\n",
      "Loss after mini batch step 146000 is : 0.463290 learning rate is : 0.090000\n",
      "Mini batch accuracy : 87.500000\n",
      "Validation accuracy : 90.320000\n",
      "Loss after mini batch step 147000 is : 0.436733 learning rate is : 0.090000\n",
      "Mini batch accuracy : 89.843750\n",
      "Validation accuracy : 90.420000\n",
      "Loss after mini batch step 148000 is : 0.503260 learning rate is : 0.090000\n",
      "Mini batch accuracy : 87.500000\n",
      "Validation accuracy : 90.310000\n",
      "Loss after mini batch step 149000 is : 0.587085 learning rate is : 0.090000\n",
      "Mini batch accuracy : 85.156250\n",
      "Validation accuracy : 90.190000\n",
      "Loss after mini batch step 150000 is : 0.463749 learning rate is : 0.090000\n",
      "Mini batch accuracy : 89.062500\n",
      "Validation accuracy : 90.160000\n",
      "Loss after mini batch step 151000 is : 0.439620 learning rate is : 0.090000\n",
      "Mini batch accuracy : 89.062500\n",
      "Validation accuracy : 90.310000\n",
      "Loss after mini batch step 152000 is : 0.554823 learning rate is : 0.090000\n",
      "Mini batch accuracy : 84.375000\n",
      "Validation accuracy : 90.270000\n",
      "Loss after mini batch step 153000 is : 0.597194 learning rate is : 0.090000\n",
      "Mini batch accuracy : 85.937500\n",
      "Validation accuracy : 90.520000\n",
      "Loss after mini batch step 154000 is : 0.410168 learning rate is : 0.090000\n",
      "Mini batch accuracy : 93.750000\n",
      "Validation accuracy : 90.590000\n",
      "Loss after mini batch step 155000 is : 0.355770 learning rate is : 0.090000\n",
      "Mini batch accuracy : 92.968750\n",
      "Validation accuracy : 90.180000\n",
      "Loss after mini batch step 156000 is : 0.503818 learning rate is : 0.090000\n",
      "Mini batch accuracy : 90.625000\n",
      "Validation accuracy : 90.410000\n",
      "Loss after mini batch step 157000 is : 0.349211 learning rate is : 0.090000\n",
      "Mini batch accuracy : 92.968750\n",
      "Validation accuracy : 90.490000\n",
      "Loss after mini batch step 158000 is : 0.538345 learning rate is : 0.090000\n",
      "Mini batch accuracy : 86.718750\n",
      "Validation accuracy : 90.230000\n",
      "Loss after mini batch step 159000 is : 0.441974 learning rate is : 0.090000\n",
      "Mini batch accuracy : 90.625000\n",
      "Validation accuracy : 90.250000\n",
      "Loss after mini batch step 160000 is : 0.357664 learning rate is : 0.090000\n",
      "Mini batch accuracy : 92.187500\n",
      "Validation accuracy : 90.090000\n",
      "Loss after mini batch step 161000 is : 0.401614 learning rate is : 0.090000\n",
      "Mini batch accuracy : 92.187500\n",
      "Validation accuracy : 90.150000\n",
      "Loss after mini batch step 162000 is : 0.459408 learning rate is : 0.090000\n",
      "Mini batch accuracy : 88.281250\n",
      "Validation accuracy : 90.480000\n",
      "Loss after mini batch step 163000 is : 0.498463 learning rate is : 0.090000\n",
      "Mini batch accuracy : 86.718750\n",
      "Validation accuracy : 90.390000\n",
      "Loss after mini batch step 164000 is : 0.450869 learning rate is : 0.090000\n",
      "Mini batch accuracy : 90.625000\n",
      "Validation accuracy : 90.300000\n",
      "Loss after mini batch step 165000 is : 0.408209 learning rate is : 0.090000\n",
      "Mini batch accuracy : 90.625000\n",
      "Validation accuracy : 90.500000\n",
      "Loss after mini batch step 166000 is : 0.365924 learning rate is : 0.090000\n",
      "Mini batch accuracy : 91.406250\n",
      "Validation accuracy : 90.250000\n",
      "Loss after mini batch step 167000 is : 0.336506 learning rate is : 0.090000\n",
      "Mini batch accuracy : 93.750000\n",
      "Validation accuracy : 90.230000\n",
      "Loss after mini batch step 168000 is : 0.437158 learning rate is : 0.090000\n",
      "Mini batch accuracy : 91.406250\n",
      "Validation accuracy : 90.530000\n",
      "Loss after mini batch step 169000 is : 0.388828 learning rate is : 0.090000\n",
      "Mini batch accuracy : 93.750000\n",
      "Validation accuracy : 90.080000\n",
      "Loss after mini batch step 170000 is : 0.414355 learning rate is : 0.090000\n",
      "Mini batch accuracy : 92.968750\n",
      "Validation accuracy : 90.280000\n",
      "Loss after mini batch step 171000 is : 0.500657 learning rate is : 0.090000\n",
      "Mini batch accuracy : 89.843750\n",
      "Validation accuracy : 90.080000\n",
      "Loss after mini batch step 172000 is : 0.469830 learning rate is : 0.090000\n",
      "Mini batch accuracy : 89.843750\n",
      "Validation accuracy : 90.110000\n",
      "Loss after mini batch step 173000 is : 0.385528 learning rate is : 0.090000\n",
      "Mini batch accuracy : 93.750000\n",
      "Validation accuracy : 90.210000\n",
      "Loss after mini batch step 174000 is : 0.470730 learning rate is : 0.090000\n",
      "Mini batch accuracy : 90.625000\n",
      "Validation accuracy : 90.180000\n",
      "Loss after mini batch step 175000 is : 0.555092 learning rate is : 0.090000\n",
      "Mini batch accuracy : 89.843750\n",
      "Validation accuracy : 90.370000\n",
      "Loss after mini batch step 176000 is : 0.443968 learning rate is : 0.090000\n",
      "Mini batch accuracy : 89.062500\n",
      "Validation accuracy : 90.400000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini batch step 177000 is : 0.466668 learning rate is : 0.090000\n",
      "Mini batch accuracy : 89.843750\n",
      "Validation accuracy : 90.330000\n",
      "Loss after mini batch step 178000 is : 0.445436 learning rate is : 0.090000\n",
      "Mini batch accuracy : 89.843750\n",
      "Validation accuracy : 90.420000\n",
      "Loss after mini batch step 179000 is : 0.435805 learning rate is : 0.090000\n",
      "Mini batch accuracy : 89.843750\n",
      "Validation accuracy : 90.430000\n",
      "Loss after mini batch step 180000 is : 0.510778 learning rate is : 0.090000\n",
      "Mini batch accuracy : 88.281250\n",
      "Validation accuracy : 90.440000\n",
      "Loss after mini batch step 181000 is : 0.413918 learning rate is : 0.090000\n",
      "Mini batch accuracy : 90.625000\n",
      "Validation accuracy : 90.440000\n",
      "Loss after mini batch step 182000 is : 0.436806 learning rate is : 0.090000\n",
      "Mini batch accuracy : 90.625000\n",
      "Validation accuracy : 90.480000\n",
      "Loss after mini batch step 183000 is : 0.496015 learning rate is : 0.090000\n",
      "Mini batch accuracy : 86.718750\n",
      "Validation accuracy : 90.300000\n",
      "Loss after mini batch step 184000 is : 0.447263 learning rate is : 0.090000\n",
      "Mini batch accuracy : 91.406250\n",
      "Validation accuracy : 90.400000\n",
      "Loss after mini batch step 185000 is : 0.432153 learning rate is : 0.090000\n",
      "Mini batch accuracy : 89.062500\n",
      "Validation accuracy : 90.340000\n",
      "Loss after mini batch step 186000 is : 0.351974 learning rate is : 0.090000\n",
      "Mini batch accuracy : 92.968750\n",
      "Validation accuracy : 90.440000\n",
      "Loss after mini batch step 187000 is : 0.492821 learning rate is : 0.090000\n",
      "Mini batch accuracy : 89.843750\n",
      "Validation accuracy : 90.230000\n",
      "Loss after mini batch step 188000 is : 0.490231 learning rate is : 0.090000\n",
      "Mini batch accuracy : 89.843750\n",
      "Validation accuracy : 90.410000\n",
      "Loss after mini batch step 189000 is : 0.469029 learning rate is : 0.090000\n",
      "Mini batch accuracy : 89.062500\n",
      "Validation accuracy : 90.420000\n",
      "Loss after mini batch step 190000 is : 0.475145 learning rate is : 0.090000\n",
      "Mini batch accuracy : 88.281250\n",
      "Validation accuracy : 90.480000\n",
      "Loss after mini batch step 191000 is : 0.532200 learning rate is : 0.090000\n",
      "Mini batch accuracy : 93.750000\n",
      "Validation accuracy : 90.190000\n",
      "Loss after mini batch step 192000 is : 0.469478 learning rate is : 0.090000\n",
      "Mini batch accuracy : 89.843750\n",
      "Validation accuracy : 90.400000\n",
      "Loss after mini batch step 193000 is : 0.485758 learning rate is : 0.090000\n",
      "Mini batch accuracy : 86.718750\n",
      "Validation accuracy : 90.300000\n",
      "Loss after mini batch step 194000 is : 0.420296 learning rate is : 0.090000\n",
      "Mini batch accuracy : 91.406250\n",
      "Validation accuracy : 90.220000\n",
      "Loss after mini batch step 195000 is : 0.448997 learning rate is : 0.090000\n",
      "Mini batch accuracy : 92.187500\n",
      "Validation accuracy : 90.410000\n",
      "Loss after mini batch step 196000 is : 0.454984 learning rate is : 0.090000\n",
      "Mini batch accuracy : 89.843750\n",
      "Validation accuracy : 90.320000\n",
      "Loss after mini batch step 197000 is : 0.555974 learning rate is : 0.090000\n",
      "Mini batch accuracy : 89.062500\n",
      "Validation accuracy : 90.230000\n",
      "Loss after mini batch step 198000 is : 0.485664 learning rate is : 0.090000\n",
      "Mini batch accuracy : 89.843750\n",
      "Validation accuracy : 90.460000\n",
      "Loss after mini batch step 199000 is : 0.474897 learning rate is : 0.090000\n",
      "Mini batch accuracy : 89.062500\n",
      "Validation accuracy : 90.230000\n",
      "Loss after mini batch step 200000 is : 0.501830 learning rate is : 0.081000\n",
      "Mini batch accuracy : 89.843750\n",
      "Validation accuracy : 90.290000\n",
      "Loss after mini batch step 201000 is : 0.432518 learning rate is : 0.081000\n",
      "Mini batch accuracy : 92.187500\n",
      "Validation accuracy : 90.420000\n",
      "Loss after mini batch step 202000 is : 0.450104 learning rate is : 0.081000\n",
      "Mini batch accuracy : 90.625000\n",
      "Validation accuracy : 90.430000\n",
      "Loss after mini batch step 203000 is : 0.386183 learning rate is : 0.081000\n",
      "Mini batch accuracy : 92.187500\n",
      "Validation accuracy : 90.440000\n",
      "Loss after mini batch step 204000 is : 0.363540 learning rate is : 0.081000\n",
      "Mini batch accuracy : 92.968750\n",
      "Validation accuracy : 90.230000\n",
      "Loss after mini batch step 205000 is : 0.485459 learning rate is : 0.081000\n",
      "Mini batch accuracy : 90.625000\n",
      "Validation accuracy : 90.180000\n",
      "Loss after mini batch step 206000 is : 0.417930 learning rate is : 0.081000\n",
      "Mini batch accuracy : 89.843750\n",
      "Validation accuracy : 90.310000\n",
      "Loss after mini batch step 207000 is : 0.379830 learning rate is : 0.081000\n",
      "Mini batch accuracy : 92.187500\n",
      "Validation accuracy : 90.500000\n",
      "Loss after mini batch step 208000 is : 0.396545 learning rate is : 0.081000\n",
      "Mini batch accuracy : 92.187500\n",
      "Validation accuracy : 90.310000\n",
      "Loss after mini batch step 209000 is : 0.507986 learning rate is : 0.081000\n",
      "Mini batch accuracy : 87.500000\n",
      "Validation accuracy : 90.480000\n",
      "Loss after mini batch step 210000 is : 0.436386 learning rate is : 0.081000\n",
      "Mini batch accuracy : 88.281250\n",
      "Validation accuracy : 90.470000\n",
      "Loss after mini batch step 211000 is : 0.468213 learning rate is : 0.081000\n",
      "Mini batch accuracy : 91.406250\n",
      "Validation accuracy : 90.310000\n",
      "Loss after mini batch step 212000 is : 0.516291 learning rate is : 0.081000\n",
      "Mini batch accuracy : 89.062500\n",
      "Validation accuracy : 90.470000\n",
      "Loss after mini batch step 213000 is : 0.342115 learning rate is : 0.081000\n",
      "Mini batch accuracy : 91.406250\n",
      "Validation accuracy : 90.430000\n",
      "Loss after mini batch step 214000 is : 0.527738 learning rate is : 0.081000\n",
      "Mini batch accuracy : 85.937500\n",
      "Validation accuracy : 90.440000\n",
      "Loss after mini batch step 215000 is : 0.374416 learning rate is : 0.081000\n",
      "Mini batch accuracy : 92.187500\n",
      "Validation accuracy : 90.570000\n",
      "Loss after mini batch step 216000 is : 0.408273 learning rate is : 0.081000\n",
      "Mini batch accuracy : 91.406250\n",
      "Validation accuracy : 90.420000\n",
      "Loss after mini batch step 217000 is : 0.374842 learning rate is : 0.081000\n",
      "Mini batch accuracy : 89.843750\n",
      "Validation accuracy : 90.380000\n",
      "Loss after mini batch step 218000 is : 0.362485 learning rate is : 0.081000\n",
      "Mini batch accuracy : 92.187500\n",
      "Validation accuracy : 90.610000\n",
      "Loss after mini batch step 219000 is : 0.384650 learning rate is : 0.081000\n",
      "Mini batch accuracy : 91.406250\n",
      "Validation accuracy : 90.440000\n",
      "Loss after mini batch step 220000 is : 0.462252 learning rate is : 0.081000\n",
      "Mini batch accuracy : 86.718750\n",
      "Validation accuracy : 90.290000\n",
      "Loss after mini batch step 221000 is : 0.455018 learning rate is : 0.081000\n",
      "Mini batch accuracy : 90.625000\n",
      "Validation accuracy : 90.520000\n",
      "Loss after mini batch step 222000 is : 0.408764 learning rate is : 0.081000\n",
      "Mini batch accuracy : 92.187500\n",
      "Validation accuracy : 90.280000\n",
      "Loss after mini batch step 223000 is : 0.454478 learning rate is : 0.081000\n",
      "Mini batch accuracy : 91.406250\n",
      "Validation accuracy : 90.370000\n",
      "Loss after mini batch step 224000 is : 0.448593 learning rate is : 0.081000\n",
      "Mini batch accuracy : 89.062500\n",
      "Validation accuracy : 90.280000\n",
      "Loss after mini batch step 225000 is : 0.380073 learning rate is : 0.081000\n",
      "Mini batch accuracy : 91.406250\n",
      "Validation accuracy : 90.410000\n",
      "Loss after mini batch step 226000 is : 0.403452 learning rate is : 0.081000\n",
      "Mini batch accuracy : 90.625000\n",
      "Validation accuracy : 90.520000\n",
      "Loss after mini batch step 227000 is : 0.451671 learning rate is : 0.081000\n",
      "Mini batch accuracy : 89.062500\n",
      "Validation accuracy : 90.590000\n",
      "Loss after mini batch step 228000 is : 0.408482 learning rate is : 0.081000\n",
      "Mini batch accuracy : 89.843750\n",
      "Validation accuracy : 90.660000\n",
      "Loss after mini batch step 229000 is : 0.512806 learning rate is : 0.081000\n",
      "Mini batch accuracy : 88.281250\n",
      "Validation accuracy : 90.450000\n",
      "Loss after mini batch step 230000 is : 0.441473 learning rate is : 0.081000\n",
      "Mini batch accuracy : 91.406250\n",
      "Validation accuracy : 90.340000\n",
      "Loss after mini batch step 231000 is : 0.612835 learning rate is : 0.081000\n",
      "Mini batch accuracy : 88.281250\n",
      "Validation accuracy : 90.450000\n",
      "Loss after mini batch step 232000 is : 0.390020 learning rate is : 0.081000\n",
      "Mini batch accuracy : 90.625000\n",
      "Validation accuracy : 90.480000\n",
      "Loss after mini batch step 233000 is : 0.486673 learning rate is : 0.081000\n",
      "Mini batch accuracy : 89.062500\n",
      "Validation accuracy : 90.420000\n",
      "Loss after mini batch step 234000 is : 0.316794 learning rate is : 0.081000\n",
      "Mini batch accuracy : 94.531250\n",
      "Validation accuracy : 90.360000\n",
      "Loss after mini batch step 235000 is : 0.355736 learning rate is : 0.081000\n",
      "Mini batch accuracy : 90.625000\n",
      "Validation accuracy : 90.340000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini batch step 236000 is : 0.579356 learning rate is : 0.081000\n",
      "Mini batch accuracy : 86.718750\n",
      "Validation accuracy : 90.330000\n",
      "Loss after mini batch step 237000 is : 0.493520 learning rate is : 0.081000\n",
      "Mini batch accuracy : 88.281250\n",
      "Validation accuracy : 90.530000\n",
      "Loss after mini batch step 238000 is : 0.326500 learning rate is : 0.081000\n",
      "Mini batch accuracy : 92.968750\n",
      "Validation accuracy : 90.340000\n",
      "Loss after mini batch step 239000 is : 0.425218 learning rate is : 0.081000\n",
      "Mini batch accuracy : 89.843750\n",
      "Validation accuracy : 90.500000\n",
      "Loss after mini batch step 240000 is : 0.374700 learning rate is : 0.081000\n",
      "Mini batch accuracy : 92.187500\n",
      "Validation accuracy : 90.530000\n",
      "Loss after mini batch step 241000 is : 0.458786 learning rate is : 0.081000\n",
      "Mini batch accuracy : 90.625000\n",
      "Validation accuracy : 90.300000\n",
      "Loss after mini batch step 242000 is : 0.537933 learning rate is : 0.081000\n",
      "Mini batch accuracy : 86.718750\n",
      "Validation accuracy : 90.570000\n",
      "Loss after mini batch step 243000 is : 0.494501 learning rate is : 0.081000\n",
      "Mini batch accuracy : 91.406250\n",
      "Validation accuracy : 90.360000\n",
      "Loss after mini batch step 244000 is : 0.447807 learning rate is : 0.081000\n",
      "Mini batch accuracy : 90.625000\n",
      "Validation accuracy : 90.270000\n",
      "Loss after mini batch step 245000 is : 0.490255 learning rate is : 0.081000\n",
      "Mini batch accuracy : 92.187500\n",
      "Validation accuracy : 90.480000\n",
      "Loss after mini batch step 246000 is : 0.518190 learning rate is : 0.081000\n",
      "Mini batch accuracy : 88.281250\n",
      "Validation accuracy : 90.390000\n",
      "Loss after mini batch step 247000 is : 0.471824 learning rate is : 0.081000\n",
      "Mini batch accuracy : 89.843750\n",
      "Validation accuracy : 90.130000\n",
      "Loss after mini batch step 248000 is : 0.335221 learning rate is : 0.081000\n",
      "Mini batch accuracy : 93.750000\n",
      "Validation accuracy : 90.420000\n",
      "Loss after mini batch step 249000 is : 0.521865 learning rate is : 0.081000\n",
      "Mini batch accuracy : 90.625000\n",
      "Validation accuracy : 90.240000\n",
      "Loss after mini batch step 250000 is : 0.397678 learning rate is : 0.081000\n",
      "Mini batch accuracy : 92.968750\n",
      "Validation accuracy : 90.500000\n",
      "Loss after mini batch step 251000 is : 0.389822 learning rate is : 0.081000\n",
      "Mini batch accuracy : 92.187500\n",
      "Validation accuracy : 90.840000\n",
      "Loss after mini batch step 252000 is : 0.407788 learning rate is : 0.081000\n",
      "Mini batch accuracy : 91.406250\n",
      "Validation accuracy : 90.190000\n",
      "Loss after mini batch step 253000 is : 0.300711 learning rate is : 0.081000\n",
      "Mini batch accuracy : 95.312500\n",
      "Validation accuracy : 90.510000\n",
      "Loss after mini batch step 254000 is : 0.591191 learning rate is : 0.081000\n",
      "Mini batch accuracy : 85.937500\n",
      "Validation accuracy : 90.400000\n",
      "Loss after mini batch step 255000 is : 0.415623 learning rate is : 0.081000\n",
      "Mini batch accuracy : 90.625000\n",
      "Validation accuracy : 90.570000\n",
      "Loss after mini batch step 256000 is : 0.458745 learning rate is : 0.081000\n",
      "Mini batch accuracy : 89.062500\n",
      "Validation accuracy : 90.310000\n",
      "Loss after mini batch step 257000 is : 0.463757 learning rate is : 0.081000\n",
      "Mini batch accuracy : 89.062500\n",
      "Validation accuracy : 90.450000\n",
      "Loss after mini batch step 258000 is : 0.465581 learning rate is : 0.081000\n",
      "Mini batch accuracy : 90.625000\n",
      "Validation accuracy : 90.520000\n",
      "Loss after mini batch step 259000 is : 0.297915 learning rate is : 0.081000\n",
      "Mini batch accuracy : 96.093750\n",
      "Validation accuracy : 90.550000\n",
      "Loss after mini batch step 260000 is : 0.452278 learning rate is : 0.081000\n",
      "Mini batch accuracy : 91.406250\n",
      "Validation accuracy : 90.470000\n",
      "Loss after mini batch step 261000 is : 0.321169 learning rate is : 0.081000\n",
      "Mini batch accuracy : 94.531250\n",
      "Validation accuracy : 90.280000\n",
      "Loss after mini batch step 262000 is : 0.597420 learning rate is : 0.081000\n",
      "Mini batch accuracy : 89.062500\n",
      "Validation accuracy : 90.490000\n",
      "Loss after mini batch step 263000 is : 0.435732 learning rate is : 0.081000\n",
      "Mini batch accuracy : 89.843750\n",
      "Validation accuracy : 90.440000\n",
      "Loss after mini batch step 264000 is : 0.364229 learning rate is : 0.081000\n",
      "Mini batch accuracy : 93.750000\n",
      "Validation accuracy : 90.310000\n",
      "Loss after mini batch step 265000 is : 0.421939 learning rate is : 0.081000\n",
      "Mini batch accuracy : 92.968750\n",
      "Validation accuracy : 90.630000\n",
      "Loss after mini batch step 266000 is : 0.411804 learning rate is : 0.081000\n",
      "Mini batch accuracy : 90.625000\n",
      "Validation accuracy : 90.470000\n",
      "Loss after mini batch step 267000 is : 0.333240 learning rate is : 0.081000\n",
      "Mini batch accuracy : 94.531250\n",
      "Validation accuracy : 90.560000\n",
      "Loss after mini batch step 268000 is : 0.480236 learning rate is : 0.081000\n",
      "Mini batch accuracy : 89.062500\n",
      "Validation accuracy : 90.480000\n",
      "Loss after mini batch step 269000 is : 0.570936 learning rate is : 0.081000\n",
      "Mini batch accuracy : 88.281250\n",
      "Validation accuracy : 90.580000\n",
      "Loss after mini batch step 270000 is : 0.362543 learning rate is : 0.081000\n",
      "Mini batch accuracy : 90.625000\n",
      "Validation accuracy : 90.240000\n",
      "Loss after mini batch step 271000 is : 0.435765 learning rate is : 0.081000\n",
      "Mini batch accuracy : 91.406250\n",
      "Validation accuracy : 90.540000\n",
      "Loss after mini batch step 272000 is : 0.402564 learning rate is : 0.081000\n",
      "Mini batch accuracy : 92.968750\n",
      "Validation accuracy : 90.190000\n",
      "Loss after mini batch step 273000 is : 0.335100 learning rate is : 0.081000\n",
      "Mini batch accuracy : 94.531250\n",
      "Validation accuracy : 90.680000\n",
      "Loss after mini batch step 274000 is : 0.300052 learning rate is : 0.081000\n",
      "Mini batch accuracy : 94.531250\n",
      "Validation accuracy : 90.420000\n",
      "Loss after mini batch step 275000 is : 0.405086 learning rate is : 0.081000\n",
      "Mini batch accuracy : 91.406250\n",
      "Validation accuracy : 90.600000\n",
      "Loss after mini batch step 276000 is : 0.458703 learning rate is : 0.081000\n",
      "Mini batch accuracy : 87.500000\n",
      "Validation accuracy : 90.680000\n",
      "Loss after mini batch step 277000 is : 0.581606 learning rate is : 0.081000\n",
      "Mini batch accuracy : 83.593750\n",
      "Validation accuracy : 90.510000\n",
      "Loss after mini batch step 278000 is : 0.413975 learning rate is : 0.081000\n",
      "Mini batch accuracy : 91.406250\n",
      "Validation accuracy : 90.500000\n",
      "Loss after mini batch step 279000 is : 0.406438 learning rate is : 0.081000\n",
      "Mini batch accuracy : 92.968750\n",
      "Validation accuracy : 90.240000\n",
      "Loss after mini batch step 280000 is : 0.403615 learning rate is : 0.081000\n",
      "Mini batch accuracy : 90.625000\n",
      "Validation accuracy : 90.460000\n",
      "Loss after mini batch step 281000 is : 0.408807 learning rate is : 0.081000\n",
      "Mini batch accuracy : 89.843750\n",
      "Validation accuracy : 90.210000\n",
      "Loss after mini batch step 282000 is : 0.493280 learning rate is : 0.081000\n",
      "Mini batch accuracy : 89.843750\n",
      "Validation accuracy : 90.570000\n",
      "Loss after mini batch step 283000 is : 0.565103 learning rate is : 0.081000\n",
      "Mini batch accuracy : 87.500000\n",
      "Validation accuracy : 90.370000\n",
      "Loss after mini batch step 284000 is : 0.310900 learning rate is : 0.081000\n",
      "Mini batch accuracy : 96.093750\n",
      "Validation accuracy : 90.500000\n",
      "Loss after mini batch step 285000 is : 0.617727 learning rate is : 0.081000\n",
      "Mini batch accuracy : 84.375000\n",
      "Validation accuracy : 90.400000\n",
      "Loss after mini batch step 286000 is : 0.468601 learning rate is : 0.081000\n",
      "Mini batch accuracy : 91.406250\n",
      "Validation accuracy : 90.510000\n",
      "Loss after mini batch step 287000 is : 0.381731 learning rate is : 0.081000\n",
      "Mini batch accuracy : 92.187500\n",
      "Validation accuracy : 90.490000\n",
      "Loss after mini batch step 288000 is : 0.593268 learning rate is : 0.081000\n",
      "Mini batch accuracy : 86.718750\n",
      "Validation accuracy : 90.390000\n",
      "Loss after mini batch step 289000 is : 0.385236 learning rate is : 0.081000\n",
      "Mini batch accuracy : 92.187500\n",
      "Validation accuracy : 90.160000\n",
      "Loss after mini batch step 290000 is : 0.406798 learning rate is : 0.081000\n",
      "Mini batch accuracy : 91.406250\n",
      "Validation accuracy : 90.710000\n",
      "Loss after mini batch step 291000 is : 0.441831 learning rate is : 0.081000\n",
      "Mini batch accuracy : 90.625000\n",
      "Validation accuracy : 90.550000\n",
      "Loss after mini batch step 292000 is : 0.533676 learning rate is : 0.081000\n",
      "Mini batch accuracy : 87.500000\n",
      "Validation accuracy : 90.610000\n",
      "Loss after mini batch step 293000 is : 0.547989 learning rate is : 0.081000\n",
      "Mini batch accuracy : 87.500000\n",
      "Validation accuracy : 90.510000\n",
      "Loss after mini batch step 294000 is : 0.362301 learning rate is : 0.081000\n",
      "Mini batch accuracy : 93.750000\n",
      "Validation accuracy : 90.440000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini batch step 295000 is : 0.341995 learning rate is : 0.081000\n",
      "Mini batch accuracy : 93.750000\n",
      "Validation accuracy : 90.480000\n",
      "Loss after mini batch step 296000 is : 0.555845 learning rate is : 0.081000\n",
      "Mini batch accuracy : 87.500000\n",
      "Validation accuracy : 90.650000\n",
      "Loss after mini batch step 297000 is : 0.480229 learning rate is : 0.081000\n",
      "Mini batch accuracy : 87.500000\n",
      "Validation accuracy : 90.380000\n",
      "Loss after mini batch step 298000 is : 0.355616 learning rate is : 0.081000\n",
      "Mini batch accuracy : 92.968750\n",
      "Validation accuracy : 90.650000\n",
      "Loss after mini batch step 299000 is : 0.378070 learning rate is : 0.081000\n",
      "Mini batch accuracy : 91.406250\n",
      "Validation accuracy : 90.470000\n",
      "Loss after mini batch step 300000 is : 0.425348 learning rate is : 0.072900\n",
      "Mini batch accuracy : 91.406250\n",
      "Validation accuracy : 90.510000\n",
      "Test accuracy : 95.260000\n"
     ]
    }
   ],
   "source": [
    "num_steps = 300001\n",
    "\n",
    "with tf.Session(graph=graph) as sess :\n",
    "    tf.global_variables_initializer().run()\n",
    "    print (\"Initialized...\")\n",
    "    \n",
    "    for step in range(num_steps) :\n",
    "        \n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        batch_data = train_dataset[offset:offset + batch_size, : ]\n",
    "        batch_labels = train_labels[offset:offset + batch_size, : ]\n",
    "        \n",
    "        feed_dict = {\n",
    "            tf_train_dataset : batch_data\n",
    "            , tf_train_labelset : batch_labels\n",
    "        }\n",
    "        \n",
    "        _, l, lr, predictions = sess.run([optimizer, loss, learning_rate, train_predictions], feed_dict)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if (step % 1000 == 0) :\n",
    "            print (\"Loss after mini batch step %d is : %f learning rate is : %f\" % (step, l, lr))\n",
    "            print (\"Mini batch accuracy : %f\" % accuracy(predictions, batch_labels))\n",
    "            print (\"Validation accuracy : %f\" % accuracy(valid_predictions.eval(), valid_labels))\n",
    "    print (\"Test accuracy : %f\" % accuracy(test_predictions.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
