{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = '../../Data/Tutorial/notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Solution 1\n",
    "---------\n",
    "\n",
    "The idea is to find all the trainable variables and calculate l2_loss using built-in `tf.nn.l2_loss` function. Then add all of them to the actual loss, so that optimizer finds a way to minimize it too. Source https://stackoverflow.com/a/38466108/5330223"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_layer_size = 250\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default() :\n",
    "    \n",
    "    # Layer 1\n",
    "    weights_layer_1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_layer_size]), name=\"weight_layer_1\")\n",
    "    biases_layer_1 = tf.Variable(tf.zeros(hidden_layer_size), name=\"biases_layer_1\")\n",
    "    \n",
    "    # Layer 2\n",
    "    weights_layer_2 = tf.Variable(tf.truncated_normal([hidden_layer_size, num_labels]), name=\"weight_layer_2\")\n",
    "    biases_layer_2 = tf.Variable(tf.zeros(num_labels), name=\"biases_layer_2\")\n",
    "    \n",
    "    def model(tf_train_dataset) : \n",
    "        out_layer_1 = tf.matmul(tf_train_dataset, weights_layer_1) + biases_layer_1\n",
    "        #dense_layer = tf.layers.dense(inputs=out_layer_1, units=hidden_layer_size, activation=tf.nn.relu)\n",
    "        return tf.matmul(tf.nn.relu(out_layer_1), weights_layer_2) + biases_layer_2\n",
    "    \n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labelset = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    \n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    logits = model(tf_train_dataset)\n",
    "    \n",
    "    #calculate l2 regularization loss\n",
    "    vars   = tf.trainable_variables()\n",
    "    lossL2 = tf.add_n([ tf.nn.l2_loss(v) for v in vars ]) * 0.001\n",
    "    \n",
    "    #add l2 regularization loss to actual loss, so that optimizer finds a way to minimize it too.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labelset)) + lossL2\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(loss)\n",
    "    train_predictions = tf.nn.softmax(logits=logits)\n",
    "    valid_predictions = tf.nn.softmax(logits=model(tf_valid_dataset))\n",
    "    test_predictions = tf.nn.softmax(logits=model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it improves the score by a bit. Same thing without regularization in assignment 2 got about 86 (<90) on test set, here it's greater than 90. See below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized...\n",
      "Loss after mini batch step 0 is : 280.232422\n",
      "Mini batch accuracy : 4.687500\n",
      "Validation accuracy : 21.140000\n",
      "Loss after mini batch step 500 is : 48.212273\n",
      "Mini batch accuracy : 66.406250\n",
      "Validation accuracy : 74.970000\n",
      "Loss after mini batch step 1000 is : 28.050369\n",
      "Mini batch accuracy : 78.125000\n",
      "Validation accuracy : 81.090000\n",
      "Loss after mini batch step 1500 is : 17.036791\n",
      "Mini batch accuracy : 82.031250\n",
      "Validation accuracy : 82.990000\n",
      "Loss after mini batch step 2000 is : 10.930830\n",
      "Mini batch accuracy : 79.687500\n",
      "Validation accuracy : 83.580000\n",
      "Loss after mini batch step 2500 is : 6.759264\n",
      "Mini batch accuracy : 79.687500\n",
      "Validation accuracy : 85.800000\n",
      "Loss after mini batch step 3000 is : 4.049703\n",
      "Mini batch accuracy : 88.281250\n",
      "Validation accuracy : 86.060000\n",
      "Test accuracy : 91.890000\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as sess :\n",
    "    tf.global_variables_initializer().run()\n",
    "    print (\"Initialized...\")\n",
    "    \n",
    "    for step in range(num_steps) :\n",
    "        \n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        batch_data = train_dataset[offset:offset + batch_size, : ]\n",
    "        batch_labels = train_labels[offset:offset + batch_size, : ]\n",
    "        \n",
    "        feed_dict = {\n",
    "            tf_train_dataset : batch_data\n",
    "            , tf_train_labelset : batch_labels\n",
    "        }\n",
    "        \n",
    "        _, l, predictions = sess.run([optimizer, loss, train_predictions], feed_dict)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if (step % 500 == 0) :\n",
    "            print (\"Loss after mini batch step %d is : %f\" % (step, l))\n",
    "            print (\"Mini batch accuracy : %f\" % accuracy(predictions, batch_labels))\n",
    "            print (\"Validation accuracy : %f\" % accuracy(valid_predictions.eval(), valid_labels))\n",
    "    print (\"Test accuracy : %f\" % accuracy(test_predictions.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized...\n",
      "Loss after mini batch step 0 is : 242.390121\n",
      "Mini batch accuracy : 15.625000\n",
      "Validation accuracy : 27.590000\n",
      "Loss after mini batch step 500 is : 46.527313\n",
      "Mini batch accuracy : 100.000000\n",
      "Validation accuracy : 75.050000\n",
      "Loss after mini batch step 1000 is : 28.218557\n",
      "Mini batch accuracy : 100.000000\n",
      "Validation accuracy : 75.110000\n",
      "Loss after mini batch step 1500 is : 17.116915\n",
      "Mini batch accuracy : 100.000000\n",
      "Validation accuracy : 75.490000\n",
      "Loss after mini batch step 2000 is : 10.387437\n",
      "Mini batch accuracy : 100.000000\n",
      "Validation accuracy : 76.110000\n",
      "Loss after mini batch step 2500 is : 6.311393\n",
      "Mini batch accuracy : 100.000000\n",
      "Validation accuracy : 76.920000\n",
      "Loss after mini batch step 3000 is : 3.845309\n",
      "Mini batch accuracy : 100.000000\n",
      "Validation accuracy : 77.990000\n",
      "Test accuracy : 84.630000\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as sess :\n",
    "    tf.global_variables_initializer().run()\n",
    "    print (\"Initialized...\")\n",
    "    \n",
    "    for step in range(num_steps) :\n",
    "        \n",
    "        offset = ((step % 10) * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        batch_data = train_dataset[offset:offset + batch_size, : ]\n",
    "        batch_labels = train_labels[offset:offset + batch_size, : ]\n",
    "        \n",
    "        feed_dict = {\n",
    "            tf_train_dataset : batch_data\n",
    "            , tf_train_labelset : batch_labels\n",
    "        }\n",
    "        \n",
    "        _, l, predictions = sess.run([optimizer, loss, train_predictions], feed_dict)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if (step % 500 == 0) :\n",
    "            print (\"Loss after mini batch step %d is : %f\" % (step, l))\n",
    "            print (\"Mini batch accuracy : %f\" % accuracy(predictions, batch_labels))\n",
    "            print (\"Validation accuracy : %f\" % accuracy(valid_predictions.eval(), valid_labels))\n",
    "    print (\"Test accuracy : %f\" % accuracy(test_predictions.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected reducing training set to 1st 10 batches overfitting arises, meaning train set gets 100% accuracy but validation and train set shows poor result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Designing network for dropout only on training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_layer_size = 250\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default() :\n",
    "    \n",
    "    # Layer 1\n",
    "    weights_layer_1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_layer_size]), name=\"weight_layer_1\")\n",
    "    biases_layer_1 = tf.Variable(tf.zeros(hidden_layer_size), name=\"biases_layer_1\")\n",
    "    \n",
    "    # Layer 2\n",
    "    weights_layer_2 = tf.Variable(tf.truncated_normal([hidden_layer_size, num_labels]), name=\"weight_layer_2\")\n",
    "    biases_layer_2 = tf.Variable(tf.zeros(num_labels), name=\"biases_layer_2\")\n",
    "    \n",
    "    def model(tf_train_dataset, training = True) : \n",
    "        out_layer_1 = tf.matmul(tf_train_dataset, weights_layer_1) + biases_layer_1\n",
    "        relu_layer = tf.nn.relu(out_layer_1)\n",
    "        if (training == True) :\n",
    "            relu_layer = tf.nn.dropout(relu_layer, keep_prob=0.5)\n",
    "        return tf.matmul(relu_layer, weights_layer_2) + biases_layer_2\n",
    "    \n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labelset = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    \n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    logits = model(tf_train_dataset)\n",
    "    \n",
    "    #calculate l2 regularization loss\n",
    "    vars   = tf.trainable_variables()\n",
    "    lossL2 = tf.add_n([ tf.nn.l2_loss(v) for v in vars ]) * 0.001\n",
    "    \n",
    "    #add l2 regularization loss to actual loss, so that optimizer finds a way to minimize it too.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labelset)) + lossL2\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(loss)\n",
    "    train_predictions = tf.nn.softmax(logits=logits)\n",
    "    valid_predictions = tf.nn.softmax(logits=model(tf_valid_dataset, False))\n",
    "    test_predictions = tf.nn.softmax(logits=model(tf_test_dataset, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now start training normally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized...\n",
      "Loss after mini batch step 0 is : 368.047760\n",
      "Mini batch accuracy : 5.468750\n",
      "Validation accuracy : 36.120000\n",
      "Loss after mini batch step 500 is : 46.534801\n",
      "Mini batch accuracy : 96.093750\n",
      "Validation accuracy : 78.720000\n",
      "Loss after mini batch step 1000 is : 28.358191\n",
      "Mini batch accuracy : 97.656250\n",
      "Validation accuracy : 79.190000\n",
      "Loss after mini batch step 1500 is : 17.160471\n",
      "Mini batch accuracy : 98.437500\n",
      "Validation accuracy : 79.810000\n",
      "Loss after mini batch step 2000 is : 10.407832\n",
      "Mini batch accuracy : 99.218750\n",
      "Validation accuracy : 80.110000\n",
      "Loss after mini batch step 2500 is : 6.344682\n",
      "Mini batch accuracy : 99.218750\n",
      "Validation accuracy : 80.350000\n",
      "Loss after mini batch step 3000 is : 3.876447\n",
      "Mini batch accuracy : 98.437500\n",
      "Validation accuracy : 80.630000\n",
      "Test accuracy : 86.790000\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as sess :\n",
    "    tf.global_variables_initializer().run()\n",
    "    print (\"Initialized...\")\n",
    "    \n",
    "    for step in range(num_steps) :\n",
    "        \n",
    "        offset = ((step % 10) * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        batch_data = train_dataset[offset:offset + batch_size, : ]\n",
    "        batch_labels = train_labels[offset:offset + batch_size, : ]\n",
    "        \n",
    "        feed_dict = {\n",
    "            tf_train_dataset : batch_data\n",
    "            , tf_train_labelset : batch_labels\n",
    "        }\n",
    "        \n",
    "        _, l, predictions = sess.run([optimizer, loss, train_predictions], feed_dict)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if (step % 500 == 0) :\n",
    "            print (\"Loss after mini batch step %d is : %f\" % (step, l))\n",
    "            print (\"Mini batch accuracy : %f\" % accuracy(predictions, batch_labels))\n",
    "            print (\"Validation accuracy : %f\" % accuracy(valid_predictions.eval(), valid_labels))\n",
    "    print (\"Test accuracy : %f\" % accuracy(test_predictions.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Little imporvement while using dropout than the previous overfit case. As we are still training on only 1st 10 batch of the data which had 77 and 84% accuracy on validation and test sets correspondingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using dropout and regularization as the previous problem. Making it deep. Adding learning rate decay And Removed the bias regularizations (don't know why but everyone seems to ignore biases on regularization, may be because it's additive) (removing bias regularization didn't improve scores very much)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_layer_size_1 = 256\n",
    "hidden_layer_size_2 = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default() :\n",
    "    \n",
    "    # Layer 1\n",
    "    weights_layer_1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_layer_size_1]), name=\"weight_layer_1\")\n",
    "    biases_layer_1 = tf.Variable(tf.zeros(hidden_layer_size_1), name=\"biases_layer_1\")\n",
    "    \n",
    "    # Layer 2\n",
    "    weights_layer_2 = tf.Variable(tf.truncated_normal([hidden_layer_size_1, hidden_layer_size_2]), name=\"weight_layer_2\")\n",
    "    biases_layer_2 = tf.Variable(tf.zeros(hidden_layer_size_2), name=\"biases_layer_2\")\n",
    "    \n",
    "    # Layer 3\n",
    "    weights_layer_3 = tf.Variable(tf.truncated_normal([hidden_layer_size_2, num_labels]), name=\"weight_layer_3\")\n",
    "    biases_layer_3 = tf.Variable(tf.zeros(num_labels), name=\"biases_layer_3\")\n",
    "    \n",
    "    def model(tf_train_dataset, training = True) : \n",
    "        out_layer_1 = tf.matmul(tf_train_dataset, weights_layer_1) + biases_layer_1\n",
    "        relu_layer1 = tf.nn.relu(out_layer_1)\n",
    "        if (training == True) :\n",
    "            relu_layer1 = tf.nn.dropout(relu_layer1, keep_prob=0.8)\n",
    "            \n",
    "        out_layer_2 = tf.matmul(relu_layer1, weights_layer_2) + biases_layer_2\n",
    "        relu_layer2 = tf.nn.relu(out_layer_2)\n",
    "        if (training == True) :\n",
    "            relu_layer2 = tf.nn.dropout(relu_layer2, keep_prob=0.6)\n",
    "        return tf.matmul(relu_layer2, weights_layer_3) + biases_layer_3\n",
    "    \n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labelset = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    \n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    logits = model(tf_train_dataset)\n",
    "    \n",
    "    #calculate l2 regularization loss\n",
    "    vars   = tf.trainable_variables()\n",
    "    lossL2 = tf.add_n([ tf.nn.l2_loss(v) for v in vars if 'bias' not in v.name ]) * 0.001\n",
    "    \n",
    "    #add l2 regularization loss to actual loss, so that optimizer finds a way to minimize it too.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labelset)) + lossL2\n",
    "    \n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    starter_learning_rate = 0.1\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                               10000, 0.95, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    train_predictions = tf.nn.softmax(logits=logits)\n",
    "    valid_predictions = tf.nn.softmax(logits=model(tf_valid_dataset, False))\n",
    "    test_predictions = tf.nn.softmax(logits=model(tf_test_dataset, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train it longer time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized...\n",
      "Loss after mini batch step 0 is : 1632.682251 learning rate is : 0.100000\n",
      "Mini batch accuracy : 13.281250\n",
      "Validation accuracy : 18.000000\n",
      "Loss after mini batch step 1000 is : 79.020828 learning rate is : 0.100000\n",
      "Mini batch accuracy : 10.937500\n",
      "Validation accuracy : 13.240000\n",
      "Loss after mini batch step 2000 is : 64.991951 learning rate is : 0.100000\n",
      "Mini batch accuracy : 14.062500\n",
      "Validation accuracy : 15.190000\n",
      "Loss after mini batch step 3000 is : 53.581562 learning rate is : 0.100000\n",
      "Mini batch accuracy : 13.281250\n",
      "Validation accuracy : 16.240000\n",
      "Loss after mini batch step 4000 is : 44.207214 learning rate is : 0.100000\n",
      "Mini batch accuracy : 14.843750\n",
      "Validation accuracy : 18.610000\n",
      "Loss after mini batch step 5000 is : 36.484596 learning rate is : 0.100000\n",
      "Mini batch accuracy : 24.218750\n",
      "Validation accuracy : 20.140000\n",
      "Loss after mini batch step 6000 is : 29.989309 learning rate is : 0.100000\n",
      "Mini batch accuracy : 31.250000\n",
      "Validation accuracy : 36.620000\n",
      "Loss after mini batch step 7000 is : 24.694485 learning rate is : 0.100000\n",
      "Mini batch accuracy : 38.281250\n",
      "Validation accuracy : 49.920000\n",
      "Loss after mini batch step 8000 is : 20.719193 learning rate is : 0.100000\n",
      "Mini batch accuracy : 39.843750\n",
      "Validation accuracy : 61.560000\n",
      "Loss after mini batch step 9000 is : 16.655632 learning rate is : 0.100000\n",
      "Mini batch accuracy : 55.468750\n",
      "Validation accuracy : 71.830000\n",
      "Loss after mini batch step 10000 is : 13.881361 learning rate is : 0.095000\n",
      "Mini batch accuracy : 54.687500\n",
      "Validation accuracy : 74.070000\n",
      "Loss after mini batch step 11000 is : 11.432419 learning rate is : 0.095000\n",
      "Mini batch accuracy : 63.281250\n",
      "Validation accuracy : 79.810000\n",
      "Loss after mini batch step 12000 is : 9.623899 learning rate is : 0.095000\n",
      "Mini batch accuracy : 66.406250\n",
      "Validation accuracy : 80.790000\n",
      "Loss after mini batch step 13000 is : 7.989908 learning rate is : 0.095000\n",
      "Mini batch accuracy : 71.093750\n",
      "Validation accuracy : 81.720000\n",
      "Loss after mini batch step 14000 is : 6.527795 learning rate is : 0.095000\n",
      "Mini batch accuracy : 81.250000\n",
      "Validation accuracy : 82.290000\n",
      "Loss after mini batch step 15000 is : 5.583303 learning rate is : 0.095000\n",
      "Mini batch accuracy : 78.125000\n",
      "Validation accuracy : 83.120000\n",
      "Loss after mini batch step 16000 is : 4.775950 learning rate is : 0.095000\n",
      "Mini batch accuracy : 77.343750\n",
      "Validation accuracy : 83.700000\n",
      "Loss after mini batch step 17000 is : 4.100458 learning rate is : 0.095000\n",
      "Mini batch accuracy : 78.125000\n",
      "Validation accuracy : 84.190000\n",
      "Loss after mini batch step 18000 is : 3.431802 learning rate is : 0.095000\n",
      "Mini batch accuracy : 77.343750\n",
      "Validation accuracy : 84.710000\n",
      "Loss after mini batch step 19000 is : 3.027543 learning rate is : 0.095000\n",
      "Mini batch accuracy : 76.562500\n",
      "Validation accuracy : 85.170000\n",
      "Loss after mini batch step 20000 is : 2.472192 learning rate is : 0.090250\n",
      "Mini batch accuracy : 82.031250\n",
      "Validation accuracy : 85.570000\n",
      "Loss after mini batch step 21000 is : 2.242486 learning rate is : 0.090250\n",
      "Mini batch accuracy : 75.781250\n",
      "Validation accuracy : 85.870000\n",
      "Loss after mini batch step 22000 is : 1.832389 learning rate is : 0.090250\n",
      "Mini batch accuracy : 84.375000\n",
      "Validation accuracy : 86.360000\n",
      "Loss after mini batch step 23000 is : 1.612151 learning rate is : 0.090250\n",
      "Mini batch accuracy : 85.156250\n",
      "Validation accuracy : 86.720000\n",
      "Loss after mini batch step 24000 is : 1.293010 learning rate is : 0.090250\n",
      "Mini batch accuracy : 89.062500\n",
      "Validation accuracy : 86.930000\n",
      "Loss after mini batch step 25000 is : 1.330459 learning rate is : 0.090250\n",
      "Mini batch accuracy : 82.031250\n",
      "Validation accuracy : 87.030000\n",
      "Loss after mini batch step 26000 is : 1.057979 learning rate is : 0.090250\n",
      "Mini batch accuracy : 86.718750\n",
      "Validation accuracy : 87.260000\n",
      "Loss after mini batch step 27000 is : 1.247526 learning rate is : 0.090250\n",
      "Mini batch accuracy : 85.156250\n",
      "Validation accuracy : 87.510000\n",
      "Loss after mini batch step 28000 is : 0.979984 learning rate is : 0.090250\n",
      "Mini batch accuracy : 85.156250\n",
      "Validation accuracy : 87.740000\n",
      "Loss after mini batch step 29000 is : 0.881026 learning rate is : 0.090250\n",
      "Mini batch accuracy : 85.156250\n",
      "Validation accuracy : 87.920000\n",
      "Loss after mini batch step 30000 is : 0.688237 learning rate is : 0.085737\n",
      "Mini batch accuracy : 90.625000\n",
      "Validation accuracy : 88.360000\n",
      "Loss after mini batch step 31000 is : 0.718987 learning rate is : 0.085737\n",
      "Mini batch accuracy : 87.500000\n",
      "Validation accuracy : 88.480000\n",
      "Loss after mini batch step 32000 is : 0.785629 learning rate is : 0.085737\n",
      "Mini batch accuracy : 82.031250\n",
      "Validation accuracy : 88.740000\n",
      "Loss after mini batch step 33000 is : 0.875210 learning rate is : 0.085737\n",
      "Mini batch accuracy : 78.125000\n",
      "Validation accuracy : 88.760000\n",
      "Loss after mini batch step 34000 is : 0.727374 learning rate is : 0.085737\n",
      "Mini batch accuracy : 84.375000\n",
      "Validation accuracy : 88.880000\n",
      "Loss after mini batch step 35000 is : 0.590360 learning rate is : 0.085737\n",
      "Mini batch accuracy : 89.062500\n",
      "Validation accuracy : 88.780000\n",
      "Loss after mini batch step 36000 is : 0.645798 learning rate is : 0.085737\n",
      "Mini batch accuracy : 85.156250\n",
      "Validation accuracy : 89.150000\n",
      "Loss after mini batch step 37000 is : 0.598605 learning rate is : 0.085737\n",
      "Mini batch accuracy : 86.718750\n",
      "Validation accuracy : 89.110000\n",
      "Loss after mini batch step 38000 is : 0.599526 learning rate is : 0.085737\n",
      "Mini batch accuracy : 86.718750\n",
      "Validation accuracy : 89.340000\n",
      "Loss after mini batch step 39000 is : 0.429032 learning rate is : 0.085737\n",
      "Mini batch accuracy : 92.187500\n",
      "Validation accuracy : 89.630000\n",
      "Loss after mini batch step 40000 is : 0.381476 learning rate is : 0.081451\n",
      "Mini batch accuracy : 93.750000\n",
      "Validation accuracy : 89.470000\n",
      "Loss after mini batch step 41000 is : 0.550756 learning rate is : 0.081451\n",
      "Mini batch accuracy : 89.062500\n",
      "Validation accuracy : 89.610000\n",
      "Loss after mini batch step 42000 is : 0.514406 learning rate is : 0.081451\n",
      "Mini batch accuracy : 86.718750\n",
      "Validation accuracy : 89.630000\n",
      "Loss after mini batch step 43000 is : 0.464123 learning rate is : 0.081451\n",
      "Mini batch accuracy : 89.843750\n",
      "Validation accuracy : 89.350000\n",
      "Loss after mini batch step 44000 is : 0.477931 learning rate is : 0.081451\n",
      "Mini batch accuracy : 88.281250\n",
      "Validation accuracy : 89.780000\n",
      "Loss after mini batch step 45000 is : 0.365957 learning rate is : 0.081451\n",
      "Mini batch accuracy : 93.750000\n",
      "Validation accuracy : 89.680000\n",
      "Loss after mini batch step 46000 is : 0.638556 learning rate is : 0.081451\n",
      "Mini batch accuracy : 83.593750\n",
      "Validation accuracy : 89.870000\n",
      "Loss after mini batch step 47000 is : 0.478445 learning rate is : 0.081451\n",
      "Mini batch accuracy : 87.500000\n",
      "Validation accuracy : 89.790000\n",
      "Loss after mini batch step 48000 is : 0.308774 learning rate is : 0.081451\n",
      "Mini batch accuracy : 95.312500\n",
      "Validation accuracy : 89.980000\n",
      "Loss after mini batch step 49000 is : 0.448806 learning rate is : 0.081451\n",
      "Mini batch accuracy : 91.406250\n",
      "Validation accuracy : 89.800000\n",
      "Loss after mini batch step 50000 is : 0.507734 learning rate is : 0.077378\n",
      "Mini batch accuracy : 88.281250\n",
      "Validation accuracy : 89.870000\n",
      "Test accuracy : 94.830000\n"
     ]
    }
   ],
   "source": [
    "num_steps = 50001\n",
    "\n",
    "with tf.Session(graph=graph) as sess :\n",
    "    tf.global_variables_initializer().run()\n",
    "    print (\"Initialized...\")\n",
    "    \n",
    "    for step in range(num_steps) :\n",
    "        \n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        batch_data = train_dataset[offset:offset + batch_size, : ]\n",
    "        batch_labels = train_labels[offset:offset + batch_size, : ]\n",
    "        \n",
    "        feed_dict = {\n",
    "            tf_train_dataset : batch_data\n",
    "            , tf_train_labelset : batch_labels\n",
    "        }\n",
    "        \n",
    "        _, l, lr, predictions = sess.run([optimizer, loss, learning_rate, train_predictions], feed_dict)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if (step % 1000 == 0) :\n",
    "            print (\"Loss after mini batch step %d is : %f learning rate is : %f\" % (step, l, lr))\n",
    "            print (\"Mini batch accuracy : %f\" % accuracy(predictions, batch_labels))\n",
    "            print (\"Validation accuracy : %f\" % accuracy(valid_predictions.eval(), valid_labels))\n",
    "    print (\"Test accuracy : %f\" % accuracy(test_predictions.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think if I would run the training longer it would get to 95+ accuracy. for 300001 I have got 95+ and of course convolutional layers would imporve it further."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
